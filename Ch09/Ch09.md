
# Full text search

RavenDB indexes are based on top of the Lucene search engine library. Lucene was created specifically to allow to do fast searches on large amount of data using advanced Information Retrieval techniques. Colloquially, this is usually called Full Search Text.

It probably wouldn't surprise you that RavenDB is really good at that. Mostly because of the use of Lucene, but also because RavenDB does a whole _lot_ of work behind the scenes to get everything just right. We already discussed in detail all the process leading up to the actual indexing stage. In this chapter, we'll discuss the behavior of Lucene and how RavenDB is using it, as well as explain how you can take advantage of this information in your own systems.

Note that this isn't meant to be an exhaustive discussion of Lucene itself. The book [Lucene in Action ](http://www.manning.com/hatcher3/) is a great resource for that, and something that we have made use of during the development of RavenDB. A better resource for the actual algorithms and options available for full text search is [Information Retrieval: Algorithms and Heuristics](http://www.amazon.com/Information-Retrieval-Algorithms-Heuristics-2nd/dp/1402030045).

The first part of this chapter will discuss the theory behind full text search briefly, and the second part will show you how to make use of that inside RavenDB.

## The magic behind full text search

The major problem with full text search is that computers are pretty bad at it. A computer can look at sorted data very quickly, but doing free form searches is quite beyond its abilities. Our sample data is shown in Table 9.1.

|Post Id|Post title |
|-------|----------|
|posts/3570|Does you application has a blog? ^[This post is from 2008, and I cringe at the number of grammatical errors in just its title. ]|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |
|posts/169859|Excerpts from the RavenDB Performance team report: Comparing Branch Tables |

: Real posts ids and titles, from [my blog ](http://ayende.com/blog).

Now, what would happen if I wanted to search for blog posts about transactions? Well, one way of doing that is to run the code in Listing 9.1.

```
{caption="{Searching for all posts about transactions}" .cs}
var results = new List<BlogPost>();
foreach(var post in GetAllPosts())
{
	if(post.Title.Contains("transactions"))
		results.Add(post);
}
return results;
```
As you can imagine, this has... issues when the size of the data grows too big. This method is called a table scan, and it is generally a very bad idea in production systems.

So, how can we do this efficiently? We create an inverted index. Such an index breaks down the content that we want to index to individual terms, which then point to the document in question.

Here is what the inverted index for the data in Table 9.1.

|Term|Post Ids |    |Term|Post Ids |
|----|--------|----|----|--------|
|a|posts/3570| |and|posts/165090 posts/164869|
|application|posts/3570||background|posts/164869|
|blog|posts/3570||branch|posts/169859|
|comparing|posts/169859||distribute|posts/167362|
|do|posts/3570||early|posts/165090|
|error|posts/165090||excerpt|posts/169859|
|fallacy|posts/167362||from|posts/169859|
|have|posts/3570||lock|posts/165090|
|locks|posts/164869||merging|posts/164869|
|of|posts/167362||performance|posts/164869 posts/169859|
|ravendb|posts/169859||release|posts/165090|
|report|posts/169859||table|posts/169859|
|team|posts/169859||the|posts/169859 posts/167362|
|thread|posts/164869||transaction|posts/164869 posts/165090 posts/167362|
|you|posts/3570|

: Inverted index for the post titles in Table 9.1

Now that we have the data in this format, we can very easily find the posts that have transaction in them by doing binary search in the sorted data in Table 9.2 and then getting the relevant post ids. Even if we have a very large amount of data, we wouldn't have to make a lot of effort to search through it.

That is the magic of O(log N) vs. O(N) operations. But the inverted index in table 9.2 is just the beginning of the work we need to do for full text queries. Our next topic for discussion is analyzers, and their role in creating the inverted index.

### Analyzers

The analyzer role is to break up a piece of text to discrete terms. A term can be a word or a phrase that we'll later search upon. A trivial example of an analyzer processing some text would be just breaking it apart on a word boundary.

For example, given the text: "Transaction merging, locks, background threads and performance", we can break it into the following discrete terms: 

* Transaction
* merging, 
* locks, 
* background
* threads
* and
* performance

That would work, but it has a few issues. Note that we still have words with punctuation in them, there are upper case words and we have a term in its plural form.

All of those are typically handled by the analyzer. In most languages, the casing of the word isn't meaningful during a search. If I'm looking for "transaction", I would expect to find documents with the term "Transaction" in them. And it is very rarely meaningful to search with punctuation, so those are stripped.

After that, each analyzer goes ahead and do its own thing. An analyzer usually have a good understanding of the language involved, because a lot of the work it needs to do relies on doing language specific operations. 

In English, such operations can involve changing term from plural form to singular form, reducing "-ing" suffix, etc. A more complex analyzer can add meaning to the terms, so when we index the word "puppy", it will also recognize that this is a document about "dog". 

This isn't the place to go into all the details about the work that analyzers are doing, but hopefully you have good grasp of what they _can_ do, and what their role in the full text search system is. They are responsible for turning a whole bunch of text into smaller terms that we can search on.

RavenDB comes with several built-in analyzers, and you can add Lucene Analyzers as you need them for your own specific use cases.

* _Default Analyzer_ - Unless you specify otherwise, this is the analyzer that RavenDB will use. This analyzer will simply lowercase the whole string. This is useful to allow case insensitive searches on a specific value. Find me all user users named 'John' would be a typical example.
* _Whitespace Analyzer_ -  This analyzer will break apart the string to be searched into discrete words, but won't try to process them any further.
* _Standard Analyzer_ - This is the default analyzer we'll use if you mark a field as Analyzed. This analyzer will break apart the string into words, remove punctuation, remove the possessive S and is smart enough to recognize numbers and email addresses and keep them as is.  

Analyzer can be specified per field, or globally for all the fields in the index. We'll see exactly how that works a bit later in this chapter.

### The querying process

Analyzers aren't applied only during indexing. They are also applied during queries. For example, if I'm querying on the index shown in Table 9.2 to find all documents that match "the transactional fallacy". What am I supposed to get?

Well, if we were trying to search for the whole query, we would obviously fail. So instead, we break the query as well into terms. In this case, the terms would be: "the", "transaction" and "fallacy". 

> **The human analyzer**
>
> In order to make things simpler, the analyzer used to create table 9.2 and to analyzer the
> query is actually yours truly. 
> I've not run it through one of the existing analyzers to give better terms for the discussion. 
>
> The standard analyzer does a good job most of the time, but it isn't so good for showing the full
> power of full text search, and other analyzers that do a better job at that will generate non word
> terms.
>
> Let us take the terms "Transactions" and "transaction". The Standard Analyzer will lower case both, 
> but will not remove the plural S suffix from the first term. Analyzers that will do that, like the 
> Porter Analyzer will take a statement such as this: "Transactions merging, locks, background threads 
> and performance" and turn it into the following terms: transact merg lock background thread and 
> perform.
> 
> As you can see, some of those terms aren't English words. That doesn't matter for searching, but it 
> does introduce an extra hurdle that people need to go over when learning about full text search. 
> Hence, my decision to use a human analyzer instead. It makes it easier to explain the _concept_, and
> you can get all the gory details of that in Lucene in Action or other Information Retrieval texts.
 

	          	the 			transaction 	fallacy
-------------	------------- 	------------- 	-------
posts/164869 					Y	
posts/165090					Y
posts/167362	Y 				Y 				Y
posts/169859	Y						 

: The results for each term in the documents.

We are now going to search Table 9.2 for each of those terms, and we'll get the following results shown in Table 9.3, which is sorted by the post id, with the matching posts are shown on the same line. Now that we have the query results, we can process them further. There are two ways that we can go about giving the user the answer. We can apply an OR modify or an AND modifier.

If we were using an AND modifier, the result of this query would be a single document: posts/167362, with the title: "The fallacy of distributed transactions".

This is an exact match of what we wanted. But a single missed term would result in us getting no results. That is generally a bad thing. So let us see what would happen when we are querying using and OR modifier. In this case, the query results would be as shown in Table 9.4.

|Post Id|Post title |
|-------|----------|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |
|posts/169859|Excerpts from the RavenDB Performance team report: Comparing Branch Tables |

: The final results of the query, using an OR modifier for all terms.

And here we get into the real conundrum of full text search, the ever present tension between the search precision and recall. 

Precision is the number of documents that matched the query that are relevant to the user. While recall is the number of relevant documents in the system that were returned out of all of the matching documents in the index.

It might be easier to look at them from the other direction. Precision is measuring how many results the users got that they didn't care about and recall is how much stuff that the users wanted to see and didn't get.

Google became really popular because they were really good at being relevant. It isn't uncommon for you to make query Google and have it find you _what you meant_ vs. what you actually searched. The first few times that happened, I was quite amazed, but then I got used to it and am now annoyed when it doesn't happen everywhere.

Full discussion of how to optimize your system for best precision and recall is out of scope for this book, unfortunately. This is fascinating topic with a lot of research and practical applications. The good news is that RavenDB is using Lucene to do the indexing, so a lot of the stuff justs works for you, and when you need to pull the heavy guns and tweak things, you can do it quite easily.

Without going too deep into the topic of optimizing search quality, let us see some simple techniques that we can use to filter out noisy results. 

#### Stop words

Take a look at table 9.2, it contains terms such as "a", "and", "has", "from", "of", "the" and "you". Those terms are _extremely_ common words in English. In fact, you would be hard pressed to find any meaningful text in English that doesn't contain them.

When we run the "the transactional fallacy" query, we got a result (posts/169859) that had nothing to do with the query at hand, just because it had the term "the" in it. This is clearly going to cause us issues.

In order to handle exactly that, we have the concept of stop words. Those are noisy words that happen all the time, and for most full text search purposes, we just filter them out. They are filtered out both
during the querying phase and during indexing, to reduce the size of the index.

If you have stop words (and for the Standard Analyzer, they come as part of the package), you cannot search for terms like "the" or "you". If you really need that, you need to configure another analyzer. You can setup the Standard Analyzer to avoid using stop words, by creating a custom analyzer and deploying it with RavenDB.

After removing the stop words from the index and the query, we are left with the results in Table 9.5.

|Post Id|Post title |
|-------|----------|
|posts/165090|Early lock release, transactions and errors |
|posts/164869|Transaction merging, locks, background threads and performance |
|posts/167362|The fallacy of distributed transactions |

: Query results without stop words.

That is much better, right? Except for a small tiny part. The most relevant result is last on the list! That is where scoring can help us.

#### Scoring query results

Usually when we make queries, we like to think in binaries term. Did this document match the query or not? And ordering the results is done by some other factor (like last update date, or the user's name, etc). 

For full text search queries, the situation is somewhat different. Instead of just being a binary true/false match for the query, the results have different level of relevancy. In the current example, we have the query "the transactional fallacy". And the data shown in Table 9.2.

Our analyzer turns that into a query on the "transaction" and "fallacy" terms, with the matches shown in Table 9.6.

				transaction 	fallacy
-----------		------------ 	-------
posts/164869 	Y				
posts/165090	Y 				
posts/167362	Y 				Y

: All matches for any term in the query, without stop words.

We got three results for this query, and now is the time to given them a score. A score is an indication of how close a match a document is to what the user has searched. An obvious way to score the results in this case would be to just give a value of 1 to any match.

What this means is that we would have the following scores:

Posts Id 		Score 		
-----------	----------- 
posts/167362	2 			 		
posts/164869 	1 			
posts/165090	1 			

Because posts/167362 is matches twice, once for "fallacy" and once for "transaction", it is scored higher, and will be the first item in the returned results.

This is roughly how document scoring works, if you are willing to squint at it hard enough. In other words, that isn't actually how it works, but it is a good lie, because it gives you a rough mental model to approximate what is actually happening. The details are more complex, of course. I'm going to just touch the very surface of the topic, and reference you to other works if you want all the gory details. The Information Retrieval book mentioned in the beginning of this chapter goes into great detail about the various ways to score results. 

We can't just score documents by the number of terms that were found, because not all terms are equal. There are two important concepts that we need to deal with here. The term frequency–inverse document frequency, usually shorten to tf–idf.

The term frequency refers to the number of times a specific term appears in a document. While the inverse document frequency refers to the number of documents that contain a specific term. 

How do we use them during scoring? For each term, we consider how common that term is in the entire index (inverse document frequency). The more common a term is, the less important it becomes. If we create an index over documents discussing RavenDB, it is fairly certain that the term "ravendb" would be quite frequent. Because of that, matches on RavenDB aren't very good, because they are so common, they are not likely to be relevant.

We then consider how many times the term appears in the specific document we are scoring. A document that mention transaction once in passing is likely to be less relevant than a document that mention them multiple times.

So there is actually quite a lot that goes into scoring the results when using full text queries. Note that even with this description, we are ignoring a lot of other factors. The query might have specified that a certain match is very important and should be boosted higher than the others, or a document in the index was created with a note that if it is any kind of match, it should be a high one, etc.

And that is quite enough with theory, we know enough to be dangerous, so let us see what we can do with this with RavenDB.

## Practical full text search with RavenDB

We are now back in our familiar Northwind database, and we want to create our first real full text search index. For that, we'll perform such search on the products. Create a new index in the studio, named it "Products/Search" and use the following index definition:

	from p in docs.Products
	select new 	{ p.Name }

Then click on the Add > Field and setup the Name field as Analyzed. The end result should look like Figure 9.1.

![Full text index on products' names.](./Ch09/Figure-01.png)

A really powerful tool to help you better understand indexing is the Terms viewer, which you can access by clicking on the Terms button. You can see some of the terms for this index in Figure 9.2.

![The generated terms for the products' names.](./Ch09/Figure-02.png)

For now, let us focus on the term "anoton". Let us see how we got there. We can go to the index and query it using:

	Name: Anton

And that will find us the following two documents:

Id 			Name
---------	-----------
products/4 	Chef Anton's Cajun Seasoning
products/5 	Chef Anton's Gumbo Mix

Knowing what you know about how full text indexing works, you now know _why_ this happened. The Standard Analyzer broke the name into discrete terms, removing the possessive S and lower casing the term. When we query, the analyzer also lower cased our term, then match the queried term with the term in the index, thereby giving us those two results.

You can play around with different queries in the studio, to see how this behaves.

### Full text search from code

As fun as it always is to play with the studio, let us switch back to code and do the exact same steps as we have just done in the studio. First, we need to create the index, as shown in Listing 9.2.

```
{caption="{Full text search index on Products}" .cs}
public class Products_Search : AbstractIndexCreationTask<Product>
{
    public Products_Search()
    {
        Map = products =>
            from product in products
            select new { product.Name };

        Index(x => x.Name, FieldIndexing.Analyzed);
    }
}
```

The only new thing here is the `Index()` method, which tell RavenDB that we need to index the Name filed as Analyzed. The options for `Index()` are:

* Default - just lower case the field as a single value.
* No - Don't index this field at all. This is sometimes use to just store a field in the index, but is very rarely used in practice.
* NotAnalyzed - index the field as is, without making any modifications. That is useful if you want to make a case _sensitive_ query.
* Analyzed - use an analyzer to index this field.

Unless specified otherwise, when Analyzed is selected, the StandardAnalyzer is used. You can control which analyzer will be used for a specific field using:

	Index(x => x.Name, FieldIndexing.Analyzed);
	Analyze(x => x.Name, "My.Lucene.Analysis.Sv." + 
		"SwedishAnalyzer, My.Lucene.Analysis");

These two lines will tell RavenDB that you want the Name field analyzed, and that the analyzer to use will be the SwedishAnalyzer.

Note that this also apply during queries. So any query made on the Name field with this configuration will have the SwedishAnalyzer run on it before it is sent for terms matching on the index.

### Behind the scenes of sorting query results

Almost as important as getting the right results, is sorting them in a meaningful order. We have previously seen how document scoring works, by finding the most relevant matches for a query. 
And this is all well and good, but it isn't actually very useful for common queries. Show me all the users by their last login time isn't something that I want to apply a match for.

RavenDB supports the `OrderBy` and `OrderByDescending` in queries, naturally, and you generally don't have to 
think about how that works. But we are interested in giving you all the information about how RavenDB works, and there are some things that you need to be aware of regarding sorting.

When a query specify that it wants to sort based on a particular field, we add another stage to the query process. We start with finding the right results for the query. The sorting action happens during that phase. 
Instead of loading all of the results to memory, then sorting them, then giving you just the number of results you wanted, the query engine is using a heap to keep only the highest ranking values for the query.

But how does this _works_? Sorting is always done in lexicographical on the raw term values. So given two terms: "apples" and "anton", we'll compare them both and rank "anton" higher than "apples". This is important, because we can't always do that successfully.

#### Sorting numbers

For string comparisons, this obviously works great. And RavenDB index dates and timespans in a format that sorts well lexicographically. But what about numbers? How would RavenDB sort the terms 50, 100, 3000 ?

Unfortunately, it would sort the following order:

#. 100
#. 3000
#. 50

The problem is that lexicographically comparing numbers doesn't work. RavenDB handles this by indexing numbers _twice_. Once as a string, to make equality comparisons easy, and the second time in a special
format that allows sorting natively. 

Let us consider the following index:
	
	from p in docs.Products
	select new { p.PricePerUser }



When RavenDB run this index, every time it encounters the `PricePerUser` field, it will first index the number in a textual format under the name `PricePerUser` and in a numeric format under the name `PricePerUser_Range`.

This means that if we want to sort the values properly on the `PricePerUser`, we actually need to sort on the `PricePerUser_Range` field.

#### The size of the number
 
The numeric format that RavenDB uses to allow proper search (and proper range queries) on numbers
it sensitive to the _size_ of the number. That isn't about the specific value in a particular document, 
but rather the type of the number. Int32 vs Int64 vs Double vs Float, etc.
 
You can specify explicitly which type RavenDB should use to store the number in the index by setting the
Sort on that field, as shown in Listing 9.3.

```
{caption="{Setting the sort order for a number}" .cs}
public class Products_ByPrices : AbstractIndexCreationTask<Product>
{
    public Products_ByPrices()
    {
        Map = products =>
            from product in products
            select new { product.PricePerUser };

        Sort(x=>x.PricePerUser, SortOptions.Double);
    }
}
```

Specifying the sort order for a field is useful only for numbers, and it will cause RavenDB to index that value with the proper type to allow searches and range queries. 

That does bring an interesting point, how does this works when we are querying? Usually, you don't have to think about it because the client API is going to take care of it all. However, if you are manually composing queries (for example, directly in the studio while you are exploring the data), you need to state this explicitly. RavenDB uses the following prefixes to denote number type during query:

* Ix - stands for Int32, example usage is:

	Age_Range: [Ix18 TO Ix25]

* Lx - stands for Int64, example usage is:

	Ticks_Range: [Lx635580000000000000 TO Lx635600000000000000]

* Fx - stands for Single (float), example usage is:
	
	Weight_Range: [Fx50.0 TO Fx55.5]

* Dx - stands for Double, example usage is:

	Price_Range: [Dx100.0 TO Dx125.80]


### Culture specific collations

The world was much simpler if we all we had was ASCII. Unfortunately, there was that little incident with the Tower of Babylon^[Which did give us Babylon 5, so maybe we are even here?] and basically no one can agree on a single language.

The problem with no single language is even worse when you consider sorting. Even when you have languages that share the same alphabet, you don't get the same _rules_^[Some of the examples below were taken from http://unicode.org/reports/tr10/].

In German, 'ö'' is smaller than 'z'. But in Swedish, the reverse is true. And if you are targeting Sweden, you can be sure to get upset calls from people who find the order of your results utterly broken.

It is actually worse, in German, the sort order in the phone book is _different_ than the sort order in a dictionary. The string 'of' is sorted before 'öf' in the dictionary, but after in the phone book. 

A lot of this stuff is absolutely arbitrary and the subject of much hair pulling for outside observers. Luckily, RavenDB already handles most of it for you. You can ask RavenDB to sort a specific field based on a particular culture collation, as shown in Listing 9.4.

```
{caption="{Setting specific collation for a field}" .cs}
public class Products_FrenchNames : AbstractIndexCreationTask<Product>
{
    public Products_FrenchNames()
    {
        Map = products =>
            from product in products
            select new { product.Name, French_Name = product.Name };

        Index("French_Name", FieldIndexing.Analyzed);
        Analyze("French_Name", 
        	"Raven.Database.Indexing.Collation" + 
        	".Cultures.FrCollationAnalyzer, Raven.Database");
    }
}
```

In order to fit it on the page, we had to break the analyzer into multiple lines. For French, you need the following string "Raven.Database.Indexing.Collation.Cultures.FrCollationAnalyzer, Raven.Database". For Swedish, you'll use SwCollationAnalyzer, etc.

Note that in Listing 9.4 we have indexed the Name field _twice_. Once without the collation analyzer, and once with it. Collation analyzers are pretty specific for sorting based on specific collation, and aren't really useful for searching. So we index such fields twice, once with and once without the collation analyzer. Queries are made against the standard field, and sorting is done on the collation field.

### Suggestions

After talking about finding the right way to sort documents, we need to talk about another issue, how to find the _right_ documents, even when users are in the way. What do I mean by that?

What would happen if I queried the `Products/Search` index we saw in Listing 9.2 and tried to query for a product name called "anisead"? I would get no results back, because there isn't any product that contains that term.

Spelling mistakes and other outright errors are pretty common when you let the user just enter free form text. On the other hand, that is what you want. No one wants to go back to the search screens with 50 fields.

That is why we have suggestions. We'll first show how we can use suggestions, and then focus on what exactly is going on here. Take a look at Listing 9.5.


```
{caption="{Getting suggestions for a misspelled word}" .cs}
public class Products_Search : AbstractIndexCreationTask<Product>
{
    public Products_Search()
    {
        Map = products =>
            from product in products
            select new { product.Name };

        Index(x => x.Name, FieldIndexing.Analyzed);
        Suggestion(x=>x.Name);
    }
}

// querying the index for suggestions
var result = documentStore.DatabaseCommands
    .Suggest("Products/Search", new SuggestionQuery
    {
        Field = "Name",
        Term = "anisead"
    });
foreach (string suggestion in result.Suggestions)
{
    Console.WriteLine(suggestion);
}
```

We are asking RavenDB to create the index `Products/Search`, and to enable suggestions for the `Name` field. Then we are querying RavenDB for suggestions to what the user has meant when they asked for a `anisead` on the `Name` field.

Running the code in Listing 9.5 should print "Aniseed" to the console. We have consulted RavenDB and it looked at our term and the currently indexed data, and it suggested the correct term Aniseed. This can be a very powerful tool to improve search quality.

#### How suggestions work?

Suggestions rely on the notion of string distance algorithms. A string distance algorithm compares two strings and find the distance between them, how different they are from one another. 

RavenDB supports the Jaro Winkler, NGram and Levenshtein algorithms, with Levenshtein being the default one used. All of those can be used to find various measures of similarity, depending on how close you want the suggestions to be.

Obviously, too strict accuracy level will lead to no results, while low level of accuracy will give irrelevant results. You might need to play around with the settings to get it working nicely.

However, there is one thing that you need to keep in mind. Suggestions are _expensive_ during the indexing phase. Frequently suggestions indexing costs will overshadow all other indexing activity in RavenDB. The reason behind that is the way suggestion indexing works.

Just having a string distance algorithm isn't enough. A string distance will work on comparing two strings, but before we can do that, we need to _find_ the second string to compare to. RavenDB does this by breaking the queried term to component parts and trying to find matches to every part of it.

But we have to deal with misspelling and errors, so during indexing, we also break the term up, and index it so we can find the various pieces of it easily. For example, indexing the term RavenDB with suggestions enabled will result in 11 terms being generated, as shown in Table 9.9.	

I changed one of the product names in the Northwind database to "RavenDB" and asked for suggestions on the string: "ravindb". I got the following results:

* ravendb
* ravioli^[I am very fond of Ravioli, so I'm not even upset about that.]

Why did we get that result? We broke apart the "ravindb" term into the components shown in Table 9.10.

gram3	gram4	start3	start4	end3	end4
-----	-----	------	------	----	----
rav  	rave 	rav   	rave  	ndb 	endb
ave  	aven 	      	      	    	
ven  	vend 	      	      	    	
endb 	     	      	      	

: The terms generated during suggestions preparation for the term "RavenDB"    

start3	end3	gram3	start4	gram4	end4
------	-----	----- 	------ 	----- 	----
rav 	ndb 	rav 	ravi	ravi	indb
				avi 			avin
				vin 			vind
				ind 			indb
				ndb

: The terms generated on the term "ravindb" during suggestion query.

We then compared each of those terms to the values that we indexed for each term's suggestions. This gives us a fast way to find all the potential matches. From there, we run the string distance algorithm to find terms that are close enough to term we are searching for. 

This process of preparing ahead of time for all the possible match is very expensive in terms of CPU time, and is a major reason why suggestions indexing can be expensive. Generally suggestions are really good when using them on small unique values, such as names or short descriptions. On longer texts, the computation cost is quite high. For example, the first sentence in this paragraph will generate 64 terms for suggestion indexing.

Note that if you are looking for a spell checker functionality, that can be provided quite efficiently elsewhere. The key benefit of suggestions is that they are working on the actual data you have in your database.

## Handling users' queries

By now you have a good idea about how indexing works, and what kind of options you have to tweak what they are doing. But we still haven't dealt with something very important, how do we actually run queries? For the rest of this chapter, we will use the index defined in Listing 9.6.

```
{caption="{The Products/Search index allows complex searches on products}" .cs}
public class Products_Search : AbstractIndexCreationTask<Product>
{
    public Products_Search()
    {
        Map = products =>
            from product in products
            select new
            {
                product.Name,
                product.Description,
                product.Price,
                product.Supplier,
                product.Category,
                product.Discontinued
            };

        Index(x => x.Name, FieldIndexing.Analyzed);
        Index(x => x.Description, FieldIndexing.Analyzed);
        Suggestion(x => x.Name);
    }
}
```

A typical model for full text searching is based on the user looking up something based on free form text. So how does this work with RavenDB? RavenDB offers the `Search()`^[You might need to add a using statement to `Raven.Client.Linq` to see it] method to handle user's queries.

A typical use case for that is shown in Listing 9.7.

```
{caption="{Performing full text search query with the RavenDB API}" .cs}
string query = Console.ReadLine();
var results = session.Query<Product, Products_Search>()
    .Search(x => x.Name, query)
    .ToList();
```

We pass the query in Listing 9.6 from the user directly to the RavenDB query, specifying what field this is relevant to, and RavenDB takes cares of everything else. Note that you don't have to cleanup the input, or process it in any way.

> **`Search()` is a phony**
> 
> Search isn't actually anything special. We could have written the query above using a `Where()` method 
> and got pretty much the same behavior. But calling `Search()` does have a specific purpose.
>
> For the person reading the code, this indicates that you are doing a full text search (and that the field 
> in question is likely Analyzed). When using a `Where()`, we tend to assume an exact match. RavenDB also 
> handles consecutive `Search()` calls slightly differently (more on that in the next section).
> 
> Just using `Search()` gives you better options for tweaking your query, such as allowing prefix and postfix 
> wildcards, or send a raw query directly to RavenDB, without any processing^[With the obvious implications of letting a user to run a random query, so handle this option with care.].
> 
> But by far the most important aspect of `Search()` is that it allows the user to define their own complex
> queries. You can just type a string like "Chef Alice" and get all the matches, but a user can also be
> smarter and issues queries such as "Chef AND NOT Alice", or any arbitrary complex operation on the field 
> in question. That let us move a lot of complexity to the hands of the users. It also means that you can
> skip building that horrible search screen UI in favor of a single text box with a "Do you 
> feel lucky?" button.

Listing 9.6 showed us making a query on a single property, but we can combine multiple `Search()` and `Where()` calls, such as in Listing 9.8.

```
{caption="{Performing complex full text search query with multiple clauses}" .cs}
var results = session.Query<Product, Products_Search>()
    .Search(x => x.Name, query, boost: 2)
    .Search(x => x.Description, query)
    .Where(x => x.Discontinued == false)
    .ToList();
```

The query in Listing 9.8 is interesting, because it combines several different behaviors in a a subtle fashion. If our query was "Chef Alice", then the query we'll send to the server would be: 

	(Name:(Chef Alice)^2 Description:(Chef Alice)) AND (Discontinued:false)

What is going on? In plain English, we are asking RavenDB to find the products with the query in them on 
either the `Name` or the `Description ` fields. But only if the product hasn't been discontinued. Note that 
we are also specifying a boost here. That means that we are asking that matches on the `Name` field would be
ranked higher than matches on `Description `.

By default, consecutive `Search()` calls are applied with an OR modifier while `Where()` calls will be ANDed.
You can control that by passing the appropriate `options` behavior to the `Search()` call, but as you can see in Listing 9.8, this behavior results in pretty good default behavior.

That is it for working with full text queries. We have on last related topic to deal with before we can see a full blown example, Facets...

## Facets

Facets allows us to present the user with analysis of the data over multiple axes (facets). This is easiest to explain using an example.

We want to search over products, and we want to show not just the results of the search, but also attributes of the entire result set. This allows the user to quickly and efficiently find what they are looking for.

We want to make a query on "Chef Alice Apples", and we want to give the user a good search experience. We want to show not just the results of the query, but also the whole structure of the entire result set. 

> **Facets run on the index**
>
> Throughout this section, we'll be using the index defined in Listing 9.6. It is important to understand
> that whenever we talk about facets, we talk about facets on the data in the _index entries_.
> That means that we cannot apply facets to anything that wasn't indexed
>  (so you can't just add a facet on a property that isn't index). 
> 
> On the same vein, if your index is outputting multiple entries per document (fan out), the facet values are
> going to be wrong, because what the facet will be counting will be the index entries, not documents.

Here is what the final results would look like, notice what happens after we are showing the first 2 results:

> Results (2 out of 5 shown)
> 
>  * Manjimup Dried Apples
>  * Chef Anton's Cajun Seasoning
> 
> Suppliers 		Categories 	 		Prices
> ---------			--------------- 	--------
> suppliers/2 : 2 	categories/2: 2 	\ 0 - \ \ 50: 4	
> suppliers/24: 1 	categories/5: 1 	50 - 100: 1
> suppliers/26: 1 	categories/6: 1
> suppliers/7 : 1 	categories/7: 1

What we are showing the user isn't just the results in the first page, but also how the total results break down based on the various suppliers, categories, and price ranges. The user can now decide to narrow down the search further by including only products within a certain price range, or by a specific supplier.

This feature is wildly used in E-Commerce systems, where users usually want to quickly narrow down a product catalog to the specific items they wish to purchase, but this is also very helpful to allow users to investigate the data dynamically, without forcing us to build very complex search screens.

Listing 9.9 shows how we can make the search results above a reality.

```
{caption="{Querying RavenDB for the query and its facets}" .cs}
 var query = session.Query<Product, Products_Search>()
     .Search(x => x.Name, "Chef Alice Apples");

 RavenQueryStatistics stats;
 var results = query
     .Statistics(out stats)
     .Take(2)
     .ToList();

Facet[] productQueryFacets = new Facet[]
    {
        new Facet<Product>
        {
            Name = x => x.Supplier
        },
        new Facet<Product>
        {
            Name = x => x.Category
        },
        new Facet<Product>
        {
            Name = x => x.Price,
            Ranges =
            {
                product => product.Price >= 0 && product.Price < 50,
                product => product.Price >= 50 && product.Price < 100,
                product => product.Price >= 100 && product.Price < 200,
                product => product.Price >= 200
            }
        }
    };


 var facetResults = query
     .ToFacets(productQueryFacets);
```

The interesting bits in Listing 9.9 isn't with the query, it is that we can apply facets to a query and get detail breakdown of the results very efficiently. We are asking RavenDB for facets on `Supplier` and `Category`, and RavenDB will return a list of the distinct suppliers and categories that matched the query, as well as the number of results that matched each distinct value.

You can also see that we defined a facet on the `Price` as well, but that is much more complex. With many numeric values, it makes little sense to try to capture individual values. Instead, we deal with ranges of values, as shown in Listing 9.9.

In this case, we divide the product based on prices from 0 - 50, 50 - 100, 100 - 200 and 200 or higher. Listing 9.10 shows how to consume the facet results.

```
{caption="{Printing the output of the faceted query result from Listing 9.9}" .cs}
Console.WriteLine("Suppliers: ");
foreach (var facetValue in facetResults.Results["Supplier"].Values)
{
    Console.WriteLine("{0}:\t{1}", facetValue.Range, facetValue.Count);
}
Console.WriteLine("Categories: ");
foreach (var facetValue in facetResults.Results["Category"].Values)
{
    Console.WriteLine("{0}:\t{1}", facetValue.Range, facetValue.Count);
}
Console.WriteLine("Prices: ");
foreach (var facetValue in facetResults.Results["Price_Range"].Values)
{
    Console.WriteLine("{0,-15}:\t{1}", facetValue.Range, facetValue.Count);
}
```

The output of Listing 9.10 is:

	Suppliers:
	suppliers/2:    		2
	suppliers/24:   		1
	suppliers/26:   		1
	suppliers/7:    		1

	Categories:
	categories/2:   		2
	categories/5:  			1
	categories/6:   		1
	categories/7:   		1

	Prices:
	{Dx0 TO Dx50]  	:       4
	{Dx50 TO Dx100]	:       1
	{Dx100 TO Dx200]:		0
	{Dx200 TO NULL]	:		0

In this case, we are dealing with fairly raw data, that we probably won't show to the user directly without some processing, but we'll deal with that topic in the next section.

Facets aren't limited to just counting the number of instances of a particular value in the query results, you can also use them for dynamic aggregation. This topic is covered in more detail in Chapter ??? (todo).

## Complete search example

We have gone over a lot of distinct aspects of RavenDB's full text and advanced queries support, but so far, we only look at each item independently. Now, we are going to create a full fledged search feature, which will allow to user to find exactly what they want easily.

We want to search for a product, by the product name or description, and we are going to apply paging, suggestions and facets to improve the search experience. We'll continue using the index shown in Listing 9.6.
Note that we indexed the product `Name` and `Description ` to allow full text search, but we only enabled suggestions on the `Name` field, which would reduce the index cost of suggestion and apply them only to the more unique terms, which is what we are usually searching on.

We already seen how we search this index, in Listing 9.8. But now we are not just doing a simple search, we are handling the full search process, which include much more than just issuing a query. 

In Listing 9.11, we use the same `productQueryFacets` value as we used in Listing 9.9.

```
{caption="{Complete search code for complex search behavior}" .cs}
public SearchResult Search(SearchQuery search)
{
    var query = session.Advanced
    	.DocumentQuery<Product, Products_Search>()
        .OpenSubclause()
            .Search(x => x.Name, search.Query).Boost(2)
            .Search(x => x.Description, search.Query)
        .CloseSubclause()
        .AndAlso()
        .WhereEquals(x => x.Discontinued, false);

    foreach (var facet in search.Facets)
    {
        query.AndAlso().Where(facet.SearchClause);
    }
           
    var lazyFacets = query.ToFacetsLazy(productQueryFacets);
    RavenQueryStatistics stats;
    var lazyQuery  = query
        .Statistics(out stats)
        .Lazily();

    session.Advanced.Eagerly.ExecuteAllPendingLazyOperations();

    if (stats.TotalResults == 0)
    {
        return SuggestMoreTerms(search);
    }

    var result = new SearchResult
    {
        TotalResults = stats.TotalResults,
        Results = lazyQuery.Value.ToList(),
    };

    AddFacetsToResults(result, lazyFacets.Value.Results);

    return result;
}
```

Quite a lot is happening here. We are using `session.Advanced.DocumentQuery()` instead of the more usual `session.Query()` because `IDocumentQuery` is much easier to manipulate grammatically than an `IQueryable`.

We setup the full text search on the name and the description as before, boosting matches on the product name higher than a match on the product description. An important part of this routine is the way we are handling the facets _from the user_. We show the facets to the user, who can select to filter the query further by one or more facets. 

We'll see how we build the SearchClause for each facet in a bit. For now, let us look at how we are actually setting up the queries. We are using Lazy to combine both the query and the facets into a single call to the database server, reducing the number of round trips that we have to make.

The call to `ExecuteAllPendingLazyOperations()` is a convenience, marking when we are actually going to the server to get the results. If we didn't call it, we would go to the server the first time the `lazyQuery` or `lazyFacets` were accessed.

We then check if we have any results. If we don't have any results, we go into `SuggestMoreTerms`, which is shown in Listing 9.12. There, we are making a suggestion query to the server, to check if the database can suggest alternative terms that the user might have been searching for.

```
{caption="{Using suggestions if we don't have any results from the query}" .cs}
private SearchResult SuggestMoreTerms(SearchQuery search)
{
    var dbCmds = session.Advanced.DocumentStore.DatabaseCommands;
    var suggest = dbCmds.Suggest("Products/Search", new SuggestionQuery
    {
        Term = search.Query,
        Field = "Name"
    });
    switch (suggest.Suggestions.Length)
    {
        case 0:
            return new SearchResult
            {
                Message = "No results for: " + search.Query,
            };
        case 1:
            return Search(new SearchQuery
            {
                Query = suggest.Suggestions[0],
                Facets = search.Facets
            });
        default:
            return new SearchResult
            {
                Message = "No results for: " + search.Query + ", did you mean...",
                Suggestions = suggest.Suggestions
            };
    }
}
```

The interesting thing about this is the way this method behaves. If there are other suggestions from the database, it just tell the user that there aren't any results. When we have more than a single suggestion, we will ask the user if he meant one of the suggestions that the database had when it saw the query.

But the interesting thing is when we have exactly one result. In this case, we don't go back to the user, we just execute the query per the database suggestion with the fixed term, and send the results back to the user. This is surprisingly effective in fixing quite a lot of minor issues in search, and this feature alone can greatly increase the effectiveness of your search function.

This query isn't just about what the user has sent. The user might have made an appropriate query, but for a product that was discontinued, or the user may have selected several facets that ended up filtering all results. Suggestions cannot help here, when we analyze a term for alternative suggestions, we don't check what other clauses in the query might have filtered results as well.

In complex situations, it might be worth it to pare down the query one clause at a time until you have more results, similar to the way Google will fix up your queries by ignoring terms that would result in no results in the query.

We request suggestions from the server only if we have no results, but if we have any results, we start applying the facets to the search result. This is done in the `AddFacetsToResults()` function, shown in Listing 9.12.

```
{caption="{Using suggestions if we don't have any results from the query}" .cs}
private void AddFacetsToResults(SearchResult result, 
    Dictionary<string, FacetResult> facets)
{
    var suppliersIds = facets["Supplier"].Values.Select(x => x.Range);
    var categoriesIds = facets["Category"].Values.Select(x => x.Range);

    session.Advanced.Lazily.Load<Supplier>(suppliersIds);
    session.Advanced.Lazily.Load<Category>(categoriesIds);

    session.Advanced.Eagerly.ExecuteAllPendingLazyOperations();

    foreach (var supplierFacetValue in facets["Supplier"].Values)
    {
        result.SuppliersFacets.Add(new FacetSearchResult
        {
            SearchClause = "Supplier: " + supplierFacetValue.Range,
            Count = supplierFacetValue.Hits,
            Name = session.Load<Supplier>(supplierFacetValue.Range).Name
        });
    }

    foreach (var supplierFacetValue in facets["Category"].Values)
    {
        result.CategoriesFacets.Add(new FacetSearchResult
        {
            SearchClause = "Category: " + supplierFacetValue.Range,
            Count = supplierFacetValue.Hits,
            Name = session.Load<Category>(supplierFacetValue.Range).Name
        });
    }

    var priceRange = facets["Price_Range"];
    var priceFacets = new[]
    {
        "Less than 50",
        "50 - 100",
        "100 - 200",
        "200 or higher"
    };
    for (int i = 0; i < 4; i++)
    {
        if (priceRange.Values[i].Hits == 0)
            continue;
        result.PriceFacets.Add(new FacetSearchResult
        {
            SearchClause = "Price_Range: " + priceRange.Values[i].Range,
            Count = priceRange.Values[i].Hits,
            Name = priceFacets[i]
        });
    }
}
```

There is a lot that is going on inside Listing 9.12. We start by getting all of the supplier and category ids that we got from the facets and load them in a single call to the server (again, using Lazy to reduce the number of round trips we make).

Then, for each supplier and category from the facets, we create a `FacetSearchResult`. We use the name of the matching document (so the user can recognize it) and also build a SearchClause, which we can add to the query to limit results to only items matching this facet.

We do the same approach for prices, but here we don't get the values from the server, but use the fixed string for the ranges we sent to the server as the name of the facet.

The end result is quite nice. When we search for the string "Chef Alice", we get the following results.

* Chef Anton's Cajun Seasoning (products/4)
* Gnocchi di nonna Alice (products/56)

Categories 					Suppliers 						Prices
----------					---------- 						----------
Condiments (1) 				New Orleans Cajun Delights (1) 	Less than 50 (2)
 - Category: categories/2 	 - Supplier: suppliers/2 		 - Price_Range: {Dx0 TO Dx50]
Grains/Cereals (1) 			Pasta Buttini s.r.l. (1)
 - Category: categories/5 	 - Supplier: suppliers/26

: Facet results when searching for Chef Alice

Try searching for "Alize", we get no results for the query, go into the suggestion code path, find a single suggestion, and then return the results for that suggestion. We just auto corrected the user and shown them the right results, with very little effort on our part.

You can start play with the facets now, passing them back to the `Search()` method to see how all of this works when we can dynamically limit the query to specific facets. This is a very powerful technique, and the whole thing takes about 100 lines of code (granted, we have no UI, but that is still impressive).

## Summary

Well, that has been quite a thrill. We started this chapter talking about how full text indexing works, by looking at how the index breaks apart the data into discrete terms to how we are able to query it efficiently. We took a peek behind the curtain by looking at scoring strategies, stop words and analyzers, seeing how they all work together to make sure sure that the results of our queries are the most relevant.

We then moved to looking at how to make use of RavenDB's full text search capabilities, by defining the Analyzed setting on specific fields. We looked at how sorting works, and discussed numerical value sorting and how we need to help RavenDB understand what is the type of number (int, long, float or double) so we can properly sort it when you need to. We discuss culture sensitive sorting with collation analyzers and some of the... funny things various languages do with sorting rules.

After sorting, we looked at suggestions, where we saw both how they work behind the scenes and how we can take advantage of this wonderful feature to give the user a much better experience. Suggestions are expensive to create during indexing, so we don't want to use them everywhere, but they are a powerful tool to use at need.

Just understanding how full text search is great, but that doesn't help without knowing how to use it. The next topic we looked at was just that, looking at the `Search()` method and its utility in allowing the user to safely send us queries that we can process against the index directly. From there we look at facets and how they allow us to slice & dice our data on various axes to give the users more tools to analyze the query results.

Finally, we brought it all together into a single feature that utilized full text search, facets, suggestions and lazy queries. I highly recommend that you'll play around with the results of this approach, because it is both very powerful and incredibly simple. RavenDB is going to do all of the heavy lifting for you, but you are going to be able to provide your users with a rich and high quality experience without undue complexity.

In the next chapter, we are going to talk about Map/Reduce. A topic that a lot of people assume is both hard and of little interest to what they need. Nothing could be further from the truth, Map/Reduce is about as simple as a group by statement, because that is exactly what it is. But I'm getting ahead of myself, turn the page, and we'll get busy discussing just this topic. 
